{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6316cf62-bb47-42db-bf9c-331701d6f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da934e57-eeea-42bc-9ff3-9df3ec6a1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvgarder le modèle\n",
    "def save_model(model, save_dir=\"../../bin\", filename=\"bert_cnn_lstm.pt\"):\n",
    "    \"\"\"\n",
    "    Sauvegarde les poids du modèle dans un fichier local.\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(save_dir, filename)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Modèle sauvegardé dans : {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e62a3d-2e27-4d5f-b95e-bf5358725b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle\n",
    "def load_model(model_class, model_kwargs, save_dir=\"../../bin\", filename=\"bert_cnn_lstm.pt\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Charge les poids d’un modèle à partir d’un fichier local.\n",
    "    \n",
    "    model_class : la classe du modèle (ex: BERT_CNN_LSTM_Classifier)\n",
    "    model_kwargs : un dictionnaire des paramètres d'initialisation (ex: {\"hidden_dim\": 768, \"num_classes\": 2})\n",
    "    \"\"\"\n",
    "    model = model_class(**model_kwargs)\n",
    "    model_path = os.path.join(save_dir, filename)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Modèle chargé depuis : {model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e40e6c-6657-44bc-a1c5-84e14c8e40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "SEQ_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "PRETRAINED_MODEL = \"hfl/chinese-roberta-wwm-ext\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1080abb2-7dba-45eb-ae6a-8a005d008d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement\n",
    "def map_to_categorical(df):\n",
    "    df['label'] = pd.Categorical(df.Tag, ordered=True).codes\n",
    "    label2Index = {row['Tag']: row['label'] for idx, row in df.iterrows()}\n",
    "    index2label = {row['label']: row['Tag'] for idx, row in df.iterrows()}\n",
    "    df.rename(columns={'label': 'labels', 'Content': 'text'}, inplace=True)\n",
    "    return df[['text', 'labels']], label2Index, index2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13fee02-c7ae-42b6-89ee-27be48d5552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/clean/steam_reviews.csv')\n",
    "df, label2Index, index2label = map_to_categorical(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4bf19b-a845-4259-8453-486bdb00de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166edce4-24ee-4c7a-a3fc-d33097d3657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, seq_len):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df['labels'].tolist()\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.seq_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43fb70e7-29c8-4fc5-b03a-434fad501308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.825 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Créer Dataset et DataLoader\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "total_size = len(df)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "df_train = df.iloc[:train_size]\n",
    "df_val = df.iloc[train_size:train_size+val_size]\n",
    "df_test = df.iloc[train_size+val_size:]\n",
    "\n",
    "# data augmentation (optionnel)\n",
    "from aug import augment_df\n",
    "df_train_aug = augment_df(df_train)\n",
    "\n",
    "train_dataset = BERTDataset(df_train_aug, tokenizer, SEQ_LEN)\n",
    "val_dataset = BERTDataset(df_val, tokenizer, SEQ_LEN)\n",
    "test_dataset = BERTDataset(df_test, tokenizer, SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4156cab0-14f1-4c19-b863-5ca0325f9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT → CNN → BiLSTM → Dense\n",
    "class BERT_CNN_LSTM_Classifier(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            if any([name.startswith(f'encoder.layer.{i}') for i in [10, 11]]) or name.startswith('pooler'):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # CNN\n",
    "        self.conv1d = nn.Conv1d(in_channels=768, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # BiLSTM on top of CNN output\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=hidden_dim,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # shape: (B, L, 768)\n",
    "\n",
    "        x = x.transpose(1, 2)  # shape: (B, 768, L) → adapter Conv1d\n",
    "        x = self.relu(self.conv1d(x))  # shape: (B, 256, L)\n",
    "        x = x.transpose(1, 2)  # shape: (B, L, 256) → transformer en LSTM \n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        pooled = lstm_out[:, -1, :]\n",
    "        x = self.dropout(pooled)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aa970f9-4074-41fd-a1fb-f5187a37f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_CNN_LSTM_Classifier(hidden_dim=768, num_classes=len(label2Index)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bae8dc0a-f70f-4e04-8feb-f9e1ae557d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "labels_np = df['labels'].to_numpy()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_np), y=labels_np)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f209db5-a8ac-42bb-885f-b8fb72688b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    # Définition de la fonction de perte pondérée selon les classes\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Optimiseur Adam avec un taux d’apprentissage initial\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    # Planificateur de taux d’apprentissage : réduction du LR toutes les 2 époques\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.5) \n",
    "\n",
    "    # Listes pour stocker l’exactitude à l'entraînement et en validation\n",
    "    train_accs, val_accs = [], []\n",
    "    train_losses = []\n",
    "    best_val_acc = 0\n",
    "    patience = 2  # Nombre d'époques à attendre avant l'arrêt anticipé\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Mode entraînement\n",
    "        total_correct, total = 0, 0\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in loop:\n",
    "            # Chargement des données sur le bon appareil (CPU/GPU)\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()  # Réinitialiser les gradients\n",
    "            outputs = model(input_ids, attention_mask)  # Prédictions du modèle\n",
    "            loss = criterion(outputs, labels)  # Calcul de la perte\n",
    "            loss.backward()  # Rétropropagation du gradient\n",
    "\n",
    "            # Éviter les gradients explosifs\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()  # Mise à jour des poids\n",
    "\n",
    "            # Calcul de la précision sur le lot actuel\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            acc = correct / labels.size(0)\n",
    "            loop.set_postfix({\n",
    "                'Batch Loss': f\"{loss.item():.4f}\",\n",
    "                'Batch Acc': f\"{acc:.4f}\"\n",
    "            })\n",
    "\n",
    "            total_correct += correct\n",
    "            total += labels.size(0)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        scheduler.step()  # Mise à jour du taux d’apprentissage\n",
    "        train_acc = total_correct / total\n",
    "        train_accs.append(train_acc)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        val_acc = total_correct / total\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Summary — Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # EarlyStopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            save_model(model, save_dir=\"../../bin\", filename=\"best_model.pt\")  # Sauvegarde du meilleur modèle\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")  # Arrêt si pas d’amélioration suffisante\n",
    "                break\n",
    "\n",
    "    return train_accs, val_accs, train_losses  # Retourne les listes d’exactitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "028dcd70-7b34-4bf4-a4ad-0e1dfe3fb80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|█| 235/235 [37:30<00:00,  9.58s/it, Batch Loss=0.0881, Batch Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary — Loss: 82.7593, Train Acc: 0.8582, Val Acc: 0.7244\n",
      "Modèle sauvegardé dans : ../../bin/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|█| 235/235 [34:32<00:00,  8.82s/it, Batch Loss=0.0529, Batch Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary — Loss: 45.2863, Train Acc: 0.9290, Val Acc: 0.7372\n",
      "Modèle sauvegardé dans : ../../bin/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|█| 235/235 [36:01<00:00,  9.20s/it, Batch Loss=0.0033, Batch Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Summary — Loss: 18.4213, Train Acc: 0.9720, Val Acc: 0.7158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|█| 235/235 [31:00<00:00,  7.92s/it, Batch Loss=0.0010, Batch Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Summary — Loss: 10.2993, Train Acc: 0.9863, Val Acc: 0.7308\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Entraînement\n",
    "train_accs, val_accs, train_losses = train(model, train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a856199f-2e4f-4613-b4d1-1032d61d0c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courbe de loss sauvegardée dans : /home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/train_loss.png\n",
      "Courbe train/val accuracy sauvegardée dans : /home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/train_val_accuracy.png\n"
     ]
    }
   ],
   "source": [
    "# Plot\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from plot.visual import plot_train_val_accuracy, plot_train_loss\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "PLOT_DIR = os.path.abspath(os.path.join(BASE_DIR, '..', 'plot'))\n",
    "\n",
    "plot_train_loss(train_losses, title=\"Train Loss over Epochs\", save_dir=PLOT_DIR)\n",
    "plot_train_val_accuracy(train_accs, val_accs, title=\"Train/Val Accuracy over Epochs\", save_dir=PLOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fe09904-5e16-4766-ac7e-a9eb6102f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Charger le modèle (si besion)\n",
    "'''\n",
    "model_kwargs = {\n",
    "    \"hidden_dim\": 768,\n",
    "    \"num_classes\": len(label2Index)\n",
    "}\n",
    "model = load_model(BERT_CNN_LSTM_Classifier, model_kwargs, device=DEVICE)\n",
    "'''\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b029894b-9f66-4c94-be7d-e5eef1c2dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3642331-8c94-41a7-9069-9c5ea780d784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion sauvegardée : /home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/test_confusion.png\n",
      "Rapport sauvegardé dans : /home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/test_classification_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangbo/miniconda3/envs/steambert/lib/python3.12/site-packages/seaborn/utils.py:61: UserWarning: Glyph 19981 (\\N{CJK UNIFIED IDEOGRAPH-4E0D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/home/yangbo/miniconda3/envs/steambert/lib/python3.12/site-packages/seaborn/utils.py:61: UserWarning: Glyph 25512 (\\N{CJK UNIFIED IDEOGRAPH-63A8}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/home/yangbo/miniconda3/envs/steambert/lib/python3.12/site-packages/seaborn/utils.py:61: UserWarning: Glyph 33616 (\\N{CJK UNIFIED IDEOGRAPH-8350}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.draw()\n",
      "/home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/visual.py:59: UserWarning: Glyph 19981 (\\N{CJK UNIFIED IDEOGRAPH-4E0D}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(cm_path)\n",
      "/home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/visual.py:59: UserWarning: Glyph 25512 (\\N{CJK UNIFIED IDEOGRAPH-63A8}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(cm_path)\n",
      "/home/yangbo/2024-2025/Outils de Traitement de Corpus/Outils-de-Traitement-de-Corpus/scrs/plot/visual.py:59: UserWarning: Glyph 33616 (\\N{CJK UNIFIED IDEOGRAPH-8350}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(cm_path)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from plot.visual import plot_evaluation\n",
    "\n",
    "plot_evaluation(\n",
    "    y_true, y_pred,\n",
    "    labels=list(index2label.values()),\n",
    "    save_dir=PLOT_DIR,\n",
    "    cm_filename=\"test_confusion.png\",\n",
    "    report_filename=\"test_classification_report.txt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
